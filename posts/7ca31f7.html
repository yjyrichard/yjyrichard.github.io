<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><div id="myscoll"></div><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>神经网络 | Yangjiayu</title><meta name="keywords" content="科普"><meta name="author" content="Yangjiayu"><meta name="copyright" content="Yangjiayu"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="神经网络科普">
<meta property="og:type" content="article">
<meta property="og:title" content="神经网络">
<meta property="og:url" content="https://yjyrichard.github.io/posts/7ca31f7.html">
<meta property="og:site_name" content="Yangjiayu">
<meta property="og:description" content="神经网络科普">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/15.png">
<meta property="article:published_time" content="2025-05-25T16:03:16.485Z">
<meta property="article:modified_time" content="2025-05-29T12:19:57.868Z">
<meta property="article:author" content="Yangjiayu">
<meta property="article:tag" content="科普">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/15.png"><link rel="shortcut icon" href="https://bilibili123.oss-cn-beijing.aliyuncs.com/about/your_image_path_here.jpg"><link rel="canonical" href="https://yjyrichard.github.io/posts/7ca31f7"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/node-snackbar/0.1.16/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"var(--theme-color)","bgDark":"#191919","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.js',
      css: 'https://cdnjs.cloudflare.com/ajax/libs/flickr-justified-gallery/2.1.2/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '神经网络',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-05-29 20:19:57'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const now = new Date()
          const hour = now.getHours()
          const isNight = hour <= 6 || hour >= 18
          if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
          else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/6.5.0/css/all.min.css"><link rel="stylesheet" href="https://cdn1.tianli0.top/npm/element-ui@2.15.6/packages/theme-chalk/lib/index.css"><style id="themeColor"></style><style id="rightSide"></style><style id="transPercent"></style><style id="blurNum"></style><style id="settingStyle"></style><span id="fps"></span><style id="defineBg"></style><style id="menu_shadow"></style><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/animate.min.css" media="print" onload="this.media='screen'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="Yangjiayu" type="application/atom+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://bilibili123.oss-cn-beijing.aliyuncs.com/about/your_image_path_here.jpg" onerror="onerror=null;src='/assets/r1.jpg'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">92</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">21</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">21</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-home"></use></svg><span class="menu_word" style="font-size:17px"> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--article"></use></svg><span class="menu_word" style="font-size:17px"> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-guidang1">                   </use></svg><span class="menu_word" style="font-size:17px"> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-sekuaibiaoqian">                   </use></svg><span class="menu_word" style="font-size:17px"> 标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-fenlei">                   </use></svg><span class="menu_word" style="font-size:17px"> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pinweishenghuo"></use></svg><span class="menu_word" style="font-size:17px"> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-yinle">                   </use></svg><span class="menu_word" style="font-size:17px"> 八音盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-dianying1">                   </use></svg><span class="menu_word" style="font-size:17px"> 影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-youxishoubing">                   </use></svg><span class="menu_word" style="font-size:17px"> 健身</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/books/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-lianjie">                   </use></svg><span class="menu_word" style="font-size:17px"> 书籍</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shejiaoxinxi"></use></svg><span class="menu_word" style="font-size:17px"> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pengyouquan">                   </use></svg><span class="menu_word" style="font-size:17px"> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-liuyan">                   </use></svg><span class="menu_word" style="font-size:17px"> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-maoliang"></use></svg><span class="menu_word" style="font-size:17px"> 个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-qunliaotian">                   </use></svg><span class="menu_word" style="font-size:17px"> 时间管理</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-love-sign">                   </use></svg><span class="menu_word" style="font-size:17px"> 恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-paperplane">                   </use></svg><span class="menu_word" style="font-size:17px"> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Yangjiayu</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page faa-parent animated-hover" href="/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-home"></use></svg><span class="menu_word" style="font-size:17px"> 首页</span></a></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon--article"></use></svg><span class="menu_word" style="font-size:17px"> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/archives/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-guidang1">                   </use></svg><span class="menu_word" style="font-size:17px"> 归档</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/tags/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-sekuaibiaoqian">                   </use></svg><span class="menu_word" style="font-size:17px"> 标签</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/categories/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-fenlei">                   </use></svg><span class="menu_word" style="font-size:17px"> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pinweishenghuo"></use></svg><span class="menu_word" style="font-size:17px"> 休闲</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/life/music/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-yinle">                   </use></svg><span class="menu_word" style="font-size:17px"> 八音盒</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/movies/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-dianying1">                   </use></svg><span class="menu_word" style="font-size:17px"> 影院</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/games/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-youxishoubing">                   </use></svg><span class="menu_word" style="font-size:17px"> 健身</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/life/books/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-lianjie">                   </use></svg><span class="menu_word" style="font-size:17px"> 书籍</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-shejiaoxinxi"></use></svg><span class="menu_word" style="font-size:17px"> 社交</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/social/fcircle/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-pengyouquan">                   </use></svg><span class="menu_word" style="font-size:17px"> 朋友圈</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/comments/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-liuyan">                   </use></svg><span class="menu_word" style="font-size:17px"> 留言板</span></a></li></ul></div><div class="menus_item"><a class="site-page group faa-parent animated-hover hide" href="javascript:void(0);"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-maoliang"></use></svg><span class="menu_word" style="font-size:17px"> 个人</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child faa-parent animated-hover" href="/personal/bb/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-qunliaotian">                   </use></svg><span class="menu_word" style="font-size:17px"> 时间管理</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/love/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-love-sign">                   </use></svg><span class="menu_word" style="font-size:17px"> 恋爱小屋</span></a></li><li><a class="site-page child faa-parent animated-hover" href="/personal/about/"><svg class="menu_icon faa-tada" aria-hidden="true" style="width:1.30em;height:1.30em;vertical-align:-0.15em;fill:currentColor;overflow:hidden;"><use xlink:href="#icon-paperplane">                   </use></svg><span class="menu_word" style="font-size:17px"> 关于</span></a></li></ul></div></div><center id="name-container"><a id="page-name" href="javascript:scrollToTop()">PAGE_NAME</a></center><div id="nav-right"><div id="search-button"><a class="search faa-parent animated-hover" title="检索站内任何你想要的信息"><svg class="faa-tada icon" style="height:24px;width:24px;fill:currentColor;position:relative;top:6px" aria-hidden="true"><use xlink:href="#icon-valentine_-search-love-find-heart"></use></svg><span> 搜索</span></a></div><a class="meihua faa-parent animated-hover" onclick="toggleWinbox()" title="美化设置-自定义你的风格" id="meihua-button"><svg class="faa-tada icon" style="height:26px;width:26px;fill:currentColor;position:relative;top:8px" aria-hidden="true"><use xlink:href="#icon-tupian1"></use></svg></a><a class="sun_moon faa-parent animated-hover" onclick="switchNightMode()" title="浅色和深色模式转换" id="nightmode-button"><svg class="faa-tada" style="height:25px;width:25px;fill:currentColor;position:relative;top:7px" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon">       </use></svg></a><div id="toggle-menu"><a><i class="fas fa-bars fa-fw"></i></a></div></div></div></nav><div id="post-info"><h1 class="post-title">神经网络</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><svg class="meta_icon post-meta-icon" style="width:30px;height:30px;position:relative;top:10px"><use xlink:href="#icon-rili"></use></svg><span class="post-meta-label">发表于 </span><time class="post-meta-date-created" datetime="2025-05-25T16:03:16.485Z" title="发表于 2025-05-26 00:03:16">2025-05-26</time><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-gengxin1"></use></svg><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-05-29T12:19:57.868Z" title="更新于 2025-05-29 20:19:57">2025-05-29</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:18px;height:18px;position:relative;top:5px"><use xlink:href="#icon-biaoqian"></use></svg><a class="post-meta-categories" href="/categories/%E7%A7%91%E6%99%AE/">科普</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:8px"><use xlink:href="#icon-charuword"></use></svg><span class="post-meta-label">字数总计:</span><span class="word-count">1.5w</span><span class="post-meta-separator">|</span><svg class="meta_icon post-meta-icon" style="width:20px;height:20px;position:relative;top:5px"><use xlink:href="#icon-shizhong"></use></svg><span class="post-meta-label">阅读时长:</span><span>43分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="神经网络"><svg class="meta_icon post-meta-icon" style="width:25px;height:25px;position:relative;top:5px"><use xlink:href="#icon-eye"></use></svg><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><section class="main-hero-waves-area waves-area"><svg class="waves-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" viewBox="0 24 150 28" preserveAspectRatio="none" shape-rendering="auto"><defs><path id="gentle-wave" d="M -160 44 c 30 0 58 -18 88 -18 s 58 18 88 18 s 58 -18 88 -18 s 58 18 88 18 v 44 h -352 Z"></path></defs><g class="parallax"><use href="#gentle-wave" x="48" y="0"></use><use href="#gentle-wave" x="48" y="3"></use><use href="#gentle-wave" x="48" y="5"></use><use href="#gentle-wave" x="48" y="7"></use></g></svg></section></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="01-从函数到神经网络">01 从函数到神经网络</h2>
<p>deepseek最近火出圈了<br>
不过可能不少人有这样的两难困境:既不想一直看一些几分钟的快餐视频.因为不论看多少,很多问题还是搞不明白,但是又不想花太多时间从头开始学,因为整个AI的知识体系实在是太庞大了。那么我们就好好的了解一下<br>
我们直接进入正题吧<br>
现在你要做的唯一一件事就是清空大脑<br>
忘掉所有你曾经熟悉的或不熟悉的概念<br>
然后在你脑海中就只留我们这趟旅行的出发点：函数<br>
后面所有一切的前提是你要相信这个世界上的所有逻辑或知识都可以用一个函数来表示<br>
“Functions Describe The world~ &quot;<br>
那我们只需要将现实世界抽象为符号,再设置好一些运算规则,也就是函数最后算出来结果,反过来解释现实世界就可以了,比如说输入直角三角形的两个边长,根据勾股定理就可以得到斜边的边长,再比如输入物体的质量和加速度<br>
根据牛顿第二定律就可以得到物体施加的力,这就是人工智能早期的思路:<strong>符号主义</strong></p>
<p>但这条路走到头了,很多问题,人类实在是想不出怎么写成一个明确的函数<br>
从上帝视角看,就是人类还是太菜了,比如说一个简简单单的识别一张图片是否是猫<br>
对人类来说可能简单到爆炸,但是要让计算机运行一段程序来识别,那一下子就变成了一个史诗级难题,就连有着明确语法规则和词典的翻译函数,尚且没有办法做到足够丝滑,那更别说复杂多变的人类智能了,那既然不知道这个函数长什么样,怎么办呢,那就别硬找了,换个思路<br>
我们先从一个简单的例子入手,比如我们知道一些X和Y的值(X:1 2 3  Y:2 4 6),我们想找到Y和X的函数关系,你有什么办法呢,有人说,这不就是Y等于2X吗,傻子都能看出来没错,这就是符号主义的思想<br>
觉得世间万物都能找到背后明确的规律<br>
但假如我们一开始没有找到这个规律,怎么办呢?<br>
比如说下面这组数字就不能一眼看出来<br>
那就用人类有史以来最具智慧的办法猜<br>
我们先把这个XY放到坐标轴上，先随便猜一下，比如说函数关系就是Y等于X<br>
也就是这里的W和B分别是一和零，然后我们一点点调整这个W和B<br>
使得这条直线越来越贴近真实数据<br>
最后呢发现完全吻合了，行就它了<br>
但有的时候可能很难找到完全吻合的函数<br>
那可怎么办呢?<br>
没事那就简化一下问题<br>
大差不差,能近似就行了,别要求那么多<br>
我们的做法仍然是一点一点调整,W和B看差不多的时候就停下来,这就是现代人工智能的思路:<strong>猜和简化问题</strong><br>
说白了实际上就是人类摆烂了,承认自己太菜了,找不到精确的函数了,那就找一个从结果上看大差不差的函数<br>
然后连蒙带猜,逐渐逼近真实答案就好了,这种区别于早期人工智能符号主义的新思想叫做<strong>连接主义</strong><br>
我们不再追求找到那个精确的函数关系，而是通过简化函数，并试图寻找一个足够接近真实答案的近似解<br>
有人说这连蒙带猜的靠谱吗？一看就不是什么正路子。没错！<br>
在连接主义成为主流之前<br>
很多人工智能的专家也是这么想的<br>
但就是这样靠连蒙带猜的办法<br>
我们居然可以用很少的参数<br>
轻松实现手写数字识别这样的任务<br>
正是这种方式在很多地方证明了它的有效性<br>
人们才开始重视起来<br>
回到正题<br>
刚刚我们举的例子都比较简单<br>
只用直线方程就可以表示了<br>
但假如数据稍稍变化一下<br>
就会发现<br>
不论怎么调整<br>
这里的W和B好像都无法接近真实的数据<br>
那这个时候就需要让这条直线弯一弯了<br>
那换句话说<br>
就是我们需要从原来的线性函数<br>
进化到非线性函数了<br>
那我们就来研究一下<br>
怎么把原来这个原本线性的函数<br>
变成非线性的呢<br>
很简单<br>
在这个函数最外层<br>
再套一个非线性的运算就可以了<br>
比如平方 比如sin 比如e<br>
这就是激活函数<br>
它的目的 就是把原本死气沉沉的线性关系给盘活了<br>
变成了变化能力更强的非线性关系嗯<br>
听到非线性关系的同学<br>
千万不要害怕<br>
常用的激活函数都简单到爆炸 但是就是能起到很好的效果<br>
好了回到这个新的函数形式<br>
我们之前仅仅有一个输入的变量<br>
就是X<br>
但实际上呢可能有很多输入<br>
所以这里的每一个X都要对应一个W<br>
像这样<br>
再者呢有的时候只套一层激活函数<br>
还是没有办法达到很好的效果<br>
也就是说这个曲线弯的还不够灵活</p>
<p><img src="https://photo.459122.xyz/i/2591bd1808bb3ddd34a656186a798bd7.png" alt="image-20250521202432295"></p>
<p>那这要怎么办呢<br>
很简单<br>
我们把刚刚这一大坨当做一个整体<br>
在此基础之上再进行一次线性变换<br>
然后再套上一个激活函数<br>
这样就可以无限的套娃下去了<br>
那通过这样的方式<br>
我们就可以构造出非常非常复杂的线性关系<br>
而且理论上可以逼近任意的连续函数</p>
<p><img src="https://photo.459122.xyz/i/b4ad5fa4d1c6200055a05d421e90cb33.png" alt="image-20250521202516116"></p>
<p>当然了<br>
这样写下去实在是太让人头大了<br>
普通人看个两层<br>
估计脑子就炸了<br>
所以我们得换一种更傻瓜的更直观的形式<br>
那回到最初的形式<br>
我们把这样一个线性变换<br>
套一个激活函数化成下面这样</p>
<p><img src="https://photo.459122.xyz/i/4d558a8b13286567809e0b1b0f3a983a.png" alt="image-20250521202555194"></p>
<p>左边是输入层<br>
只有一个输入X<br>
右边是输出层<br>
只有一个输出Y<br>
我们把这里的每一个小圈圈叫做一个神经元<br>
当然这里我不建议你把它跟生物的神经元<br>
先类比<br>
因为他们两个其实一毛钱关系都没有<br>
看似很形象<br>
但实际上反而会影响理解<br>
总之就是这样<br>
两个圈圈一连就表示上面一个函数关系<br>
那刚刚我们说输入可能有多个<br>
所以对应的变化就是输入层变成了多个<br>
像这样<br>
我们还说可以继续在外层不断的套线性变换<br>
再套激活函数<br>
那么每套一层就相当于神经元水平方向<br>
又扩展了一个<br>
当然扩展之后呢<br>
中间这一层就不再是最终的输出了<br>
而是包裹在了一个很复杂的函数变换之中<br>
看不到<br>
我们管它叫做隐藏层<br>
而整个这一大坨神经元互相连接<br>
形成的网络结构就叫做神经网络</p>
<p><img src="https://photo.459122.xyz/i/b260dc3ea01c7336b6803e720650d0ea.png" alt="image-20250521202651748"></p>
<p>接下来我们看一下函数和神经网络的对应关系<br>
首先有两个输入变量<br>
一个是X1<br>
另一个是X2<br>
它们构成了输入层<br>
然后X1X2进行一次线性变换<br>
再进行一次激活函数<br>
就得到了隐藏层a<br>
这个a对应的就是上面这一大坨表达式<br>
那我们把它当做一个整体<br>
继续进行一次线性变换和一次激活函数<br>
这就计算出了最终的输出层y<br>
重新再看一下这个过程<br>
从神经网络的这个图来看的话<br>
似乎就像是一个信号<br>
从左到右传播了过去<br>
那这个过程就叫做神经网络的前向传播</p>
<p><img src="https://photo.459122.xyz/i/e72c2edbe2035287e97f3527205c50ab.png" alt="image-20250521202743469"></p>
<p>但是实际上呢就是一点点分步骤<br>
把一个函数的值计算出来了而已<br>
神经网络的每一层神经元都可以无限增加<br>
同时呢隐藏层的层数也可以无限增加<br>
那进而就可以构成一个<br>
非常非常复杂的非线性函数了<br>
虽然这个函数可能非常复杂<br>
但是我们的目标却非常简单和明确<br>
就是根据已知的一组X和Y的值<br>
猜出所有这里的W和B都是多少</p>
<p><img src="https://photo.459122.xyz/i/e4a5dd94bac21e01407203ecfa26e4b8.png" alt="image-20250521202823501"></p>
<p>当然了<br>
我们一开始举的例子非常简单<br>
光靠肉眼法就能慢慢猜出答案了<br>
但是现在有这么多参数可能就无法凭感觉猜了<br>
那这要怎么办呢<br>
欲知后事如何<br>
且听下回分解</p>
<p>回顾一下这个内容非常简单<br>
我们从一个最开始的信念函数开始讲起<br>
早期的人工智能<br>
相信可以找到精确的函数来表示一切<br>
但因为这个世界实在太复杂了<br>
所以人们就放弃了转向寻找一个足够接近真实答案的近似解<br>
那我们通过寻找一个线性关系来举例,如何去猜测W和B的值<br>
后来发现线性关系太过简单,不足以描述更复杂的关系,于是引入了非线性的激活函数<br>
通过线性变换和非线性激活函数的不断组合<br>
和套娃可以表达很复杂的关系,但是呢写成函数看太恶心了,所以就化成了神经网络这种形式<br>
那恭喜你<br>
从函数到神经网络的这条路已经被你搞懂了<br>
剩下的所有乱七八糟的知识<br>
都仅仅是为了算出这个W和B而已</p>
<h2 id="02-如何计算神经网络的参数">02 如何计算神经网络的参数</h2>
<p>我们直接进入今天的主题<br>
看看如何计算这个W和B<br>
那我们先别搞那么复杂的非线性函<br>
先从最简单的一个线性函数入手<br>
首先第一个问题就是什么样的W和B是好的？<br>
答案其实很简单<br>
我们的目标是让这个函数的输出结果尽可能地接近真实数据<br>
因此好的W和B就是能够使得函数的尽可能拟和真实数据的那一组参数<br>
那接下来第二个问题<br>
什么叫拟合得好<br>
先别想那么多</p>
<p><img src="https://photo.459122.xyz/i/812626a99c522fae808f652147eb7a7a.png" alt="image-20250521203531802"></p>
<p>从直觉上理解这条线就拟合得挺好<br>
左边的这个就拟合得不好<br>
那第三个问题就自然浮出水面了<br>
怎么用数学语言表达<br>
刚刚我说的这个直觉上的理解呢<br>
很简单<br>
我们可以在每个数据点上画一条竖<br>
使其与拟合的直线相交<br>
由于这里的每个点的纵坐标<br>
表示的就是真实数据<br>
我们用Y来表示落在直线上的点<br>
表示预测数据<br>
我们用y hat来表示</p>
<p><img src="https://photo.459122.xyz/i/31b5289a3eb30bddd10160004a8f9e05.png" alt="image-20250521203624697"></p>
<p>那么这条线段的长度<br>
就是真实值与预测值的误差<br>
为了评估整体的拟合效果<br>
我们可以将所有这些线段的长度加<br>
这样呢就得到了预测数据与真实数之间的总的差异<br>
也就可以反映当前这个线性函数与真实数据的拟合度</p>
<p><img src="https://photo.459122.xyz/i/2e830b0c074b9a19f038d9f2091b1434.png" alt="image-20250521203704257"></p>
<p>而这个表示预测数据与真实数据误<br>
我们叫它损失函数<br>
我们着重看一下这个公式<br>
这个绝对值有些讨厌<br>
数学优化时不太友好<br>
我们做数学题时<br>
往往最讨厌碰到这种带绝对值的问<br>
要各种分类讨论啊<br>
想想就头疼<br>
所以我们改造下<br>
用平方来代替<br>
一来呢解决了绝对值不平滑的问题<br>
二来呢也放大了误差较大的值的影<br>
然后我们再根据样本的数量平均一下<br>
消除样本数量大小的影响<br>
那最终得到这个公式就叫做均方误差</p>
<p><img src="https://photo.459122.xyz/i/dee5f4ea2a4986b563822638db9f57a3.png" alt="image-20250521203752879"></p>
<p>而均方误差就是用来表示损失函数一种<br>
我们把损失函数记作L那从参数的视<br>
它就是一个关于W和B的一个函数</p>
<p><img src="./%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%90%E7%A7%91%E6%99%AE%E5%90%91%E3%80%91.assets/image-20250521203819061.png" alt="image-20250521203819061"></p>
<p>好先不要过于陷入这个公式的细节<br>
那还记得我们要干啥</p>
<p><img src="https://photo.459122.xyz/i/df55c72c143d616a4ec047a1629e4de4.png" alt="image-20250521203853848"></p>
<p>损失函数表示的是真实值与预测差距<br>
而我们的目的呢就是让这个误差最小的w和b<br>
也就是找到可以让这个损失函数L最小<br>
那个W和B那怎么求解呢<br>
自然就是用我们初中就学过的<br>
让其导数等于零求极值点的过程<br>
我们先不上公式<br>
通过一个具体的例子来说<br>
假设我们就四个样本数据<br>
就是简简单单的11223344这样<br>
然后我们的线性模型也简单点<br>
把B去掉<br>
只保留一个W<br>
其实呢就是一个简单的经过原点的一条直线Y等于WX这个时候我们展开一下损失函数<br>
把y heat值代入进来<br>
把求和符号展开<br>
然后再把上面这组XY的数据带进来<br>
平方展开再化简好<br>
那化简之后我们就可以清晰的看到<br>
这就是一个简简单单的关于W的一个<br>
接下来对W求导<br>
再让其导数等于零<br>
就可以求出W等于一了</p>
<p><img src="./%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E3%80%90%E7%A7%91%E6%99%AE%E5%90%91%E3%80%91.assets/image-20250521204022426.png" alt="image-20250521204022426"></p>
<p>代入回原直线函数<br>
此时Y等于X就是让损失函数最小<br>
也就是最拟合真实数据的那条直线</p>
<p>回过头再来看一下这个损失函数<br>
它实际上呢是一条开口向上的抛物<br>
刚刚其实就在寻找这个最低点<br>
采用的办法就是让导数等于零<br>
不过我们此时的模型是简单的线性函数<br>
而且只保留了W而忽略了B<br>
而如果此时把B算进来<br>
那就需要求关于W和B两个变量的<br>
损失函数的最小值了<br>
这个时候损失函数的图像就是一个三维图像是一个开口向上的这个碗状形状</p>
<p><img src="https://photo.459122.xyz/i/e791c7d75a305bc04965139af2642931.png" alt="image-20250521204113092"></p>
<p>我们的目标同样也是找到这个二元函数最小值<br>
所对应的那个W和B<br>
而多元函数求最小值的问题就不再是倒数了<br>
而是要让每个参数的偏导数等于零<br>
偏导数在这里就不展开了<br>
不过可能不少人学偏导数的时候都<br>
其实很简单<br>
对W求偏导<br>
就是把B当作常数和一元函数求导就<br>
在三维图像中就是这样<br>
相当于只看到这个切面上的二维函<br>
同样对于B也是如此。<br>
那么刚刚这个通过寻找一个线性函<br>
来拟和X和Y的关系<br>
就是机器学习中最基本的一种分析方法：<strong>线性回归</strong><br>
回到之前讲的神经网络<br>
神经网络是一个通过线性函数和非线性激活函数不断组合形成的一个非常复杂的非线性函数<br>
那么它对应的损失函数则是更复杂的非线性函数<br>
往往呢不能像刚刚那样通过让导数等于零直接求出最小值<br>
那怎么办呢？<br>
人们的解决办法也非常的简单粗暴<br>
就是一点点试<br>
具体怎么试呢？<br>
假如此时W和B的值均为五<br>
损失函数计算结果是十<br>
我们来看一下<br>
尝试把W增加一个单位变成六<br>
再次计算损失函数<br>
发现结果是九<br>
那就说明W的这次调整是对的<br>
让损失函数变小了<br>
再尝试把B增加一变成六<br>
发现损失函数增加了二<br>
变成了11<br>
说明B的增大会让误差变大<br>
那我们就反过来把B往小的调整<br>
让损失函数继续变小<br>
那如此循环往复不断调整就可以了<br>
总之我们每次都看下当前状况<br>
调整W或B对损失函数的影响<br>
然后每次呢把参数向着让损失函数</p>
<p><img src="https://photo.459122.xyz/i/0873e5a9500bf7c4f1084b7eb6d2787c.png" alt="image-20250521204322611"></p>
<p>那个方向调整一点点<br>
直到让损失函数足够小，那具体怎么调？</p>
<p>回到最初始的状态<br>
W变化一点点，使得损失函数会变化多少？<br>
这其实就是损失函数对W的偏导数<br>
对B来说同样也是如此<br>
而我们要做的就是<br>
让W和B不断往偏导数的反方向去变<br>
那具体变化的快慢呢<br>
再增加一个系数来控制<br>
我们叫它学习率</p>
<p><img src="https://photo.459122.xyz/i/f25c33cbf0aba3f8acc8a4aaeab96541.png" alt="image-20250521204416292"></p>
<p>这些偏导数所构成的向量就叫做梯度</p>
<p><img src="https://photo.459122.xyz/i/05bfff2a58aeabff84e6e08442ed5e1c.png" alt="image-20250521204500187"></p>
<p>而不断变化W和B让损失函数逐渐减小的一个过程，进而求出最终的W和B，这个过程就叫做梯度下降<br>
嗯不少材料在介绍梯度下降时<br>
可能经常会用到人们下山这个过程<br>
但我倒是觉得直接看公式也挺直观<br>
没必要好了<br>
这个公式很好理解<br>
但是关键就在于这个偏导数该怎么求？<br>
在之前的线性回归问题中<br>
偏导数就是一个一元二次函数<br>
求导非常简单<br>
但是在神经网络中<br>
函数本身就是一个复杂到变态的非线性函数<br>
那更别说损失函数了<br>
直接求导可能就不太好求了<br>
那怎么办呢<br>
其实也很简单<br>
虽然神经网络整体所代表的函数很复杂<br>
但是层与层之间的关系却是非常简单<br>
我们就用上面这个麻雀虽小<br>
但五脏俱全的一个简单的神经网络来举例子<br>
只有一个输入和一个输出<br>
而且中间的隐藏层也只有一个神经元<br>
首先我们根据输入X的值计算出隐藏层a的值<br>
这里的G就是随便一个激活函数<br>
比如说sigma的无所谓<br>
然后呢再根据A的值计算出输出层y hat<br>
然后再根据y hat的值以及真实值Y计算出损失函数<br>
那这里损失函数就用均方误差了<br>
而且由于只有一个输出数据<br>
所以说我把求和符号也省去了<br>
这个神经网络结构中一共有四个参<br>
要通过梯度下降的方式逐渐求解<br>
那之前也说了<br>
关键问题就是求出L对他们的偏导数<br>
那我们直接拿最难的w1来举例<br>
如何求出L对W的偏导数呢<br>
其实很简单<br>
从偏导数要表达的意思出发一下就能想明白<br>
其实我们就想看w1变化一点点<br>
会使得L变化多少了<br>
那我们就先看W1变化<br>
一个单位会使得A变化多少<br>
再看看a变化一个单位会使得y head<br>
然后再看y hat的变化<br>
一个单位会使L变化多少<br>
每一个都是一个简单的偏导数<br>
那把这三者乘起来<br>
就知道W变化一个单位会使得L变化多少了</p>
<p><img src="https://photo.459122.xyz/i/066be2071a653e646e97431e57108646.png" alt="image-20250521204746002"></p>
<p>如果实在想不明白的话<br>
可以联想一下齿轮怎么计算</p>
<p><img src="https://photo.459122.xyz/i/53efd157167bf7998edb304d4ed889c9.png" alt="image-20250521204830024"></p>
<p>第一个转一圈会使得最后一个齿轮<br>
其实就是乘起来吧<br>
那这种偏导数的计算方式就叫做链式法则<br>
其实就是微积分中的复合函数求导<br>
由于我们可以从右向左依次求导<br>
然后逐步更新每一层的参数<br>
直到把所有的神经网络的参数都更新一次<br>
在计算前一层时用到的一些偏导数<br>
后面也会用到<br>
所以说不用计算那么多<br>
而是让这些值从右向左一点点传播<br>
我们把这样一个过程形象地称之为<strong>反向传播</strong><br>
那结合之前的知识<br>
我们通过前向传播<br>
根据输入X计算出输出Y<br>
然后再通过反向传播计算出损失函树<br>
关于每个参数的梯度</p>
<p><img src="https://photo.459122.xyz/i/de8a083980b8a229087678bca97fecf0.png" alt="image-20250521204925053"></p>
<p>然后每个参数都向着梯度的反方向<br>
这就构成了神经网络的一次训练<br>
而神经网络经过多轮这样的训练<br>
里面的参数都一点一点的变化<br>
直到让损失函数足够小</p>
<p><img src="https://photo.459122.xyz/i/febf9188f23760a3962fedf5a5cda9c6.png" alt="image-20250521204944396"></p>
<p>也就是找到了我们想要的那个函数<br>
虽然听起来很简单<br>
但是面对真实问题时<br>
往往却存在着各种各样的难题<br>
具体会遇到什么样的难题<br>
又该采取什么样的办法去解决呢<br>
欲知后事如何<br>
且听下回分解<br>
回顾一下：<br>
为了找到一组W和B来拟合真实数据<br>
我们定义了损失函数<br>
并且通过制定让损失函数最小化这<br>
来计算W和B的值<br>
接下来我们通过简单的线性回归问题<br>
可以直接让损失函数的导数等于零<br>
一步就求到W和B<br>
但神经网络的复杂性<br>
让我们没有办法直接得到W和B的解<br>
只能通过一点点往偏导数的反方向<br>
调整每个参数来慢慢逼近真实答案<br>
这个方法就叫做梯度下降<br>
而由于神经网络的层数较多<br>
直接求偏导数非常困难<br>
因此可以逐层求导<br>
间接得到最终的偏导数<br>
这就是链式法则<br>
而通过链式法则求导并逐层更新参<br>
这个过程就叫做反向传播<br>
那不断的前向传播，反向传播，这就构成了神经网络的训练过程</p>
<h2 id="03-调教神经网络怎么就这么难呢？">03 调教神经网络怎么就这么难呢？</h2>
<p>我们知道神经网络的本质就是线性变换套上一个激活函数不断组合而成的一个非常复杂的非线性函数，并且巧妙地通过梯度下降一点一点地计算出神经网络中一组合适的参数，那这样看起来其实不是神经网络只有足够大，什么问题都能解决了？</p>
<p><img src="https://photo.459122.xyz/i/91e559774a4b634c08277ca28d4ac16a.png" alt="image-20250521205132364"></p>
<p>理论上是这样。</p>
<p>这一幅图你觉得那边拟合的好呢?</p>
<p><img src="https://photo.459122.xyz/i/1f0f11b4aab13e4f4539818886aa4a13.png" alt="image-20250521205418285"></p>
<p>如果从损失值最小来看，右边的好，但是根据直觉来看右边这个好像好的有点太过了。结果可能是只适合训练数据，对于新数据的预测反而不如左边的准</p>
<p><img src="https://photo.459122.xyz/i/1afada9fb751cc38cd4f8e43a1f6ce2c.png" alt="image-20250521205559524"></p>
<p>这种在训练数据上表现得很完美但是在测试数据上表现得很糟糕的现象我们叫它过拟合。而在没见过的数据上的表现能力我们叫它泛化能力。那为什么会过拟合呢？是因为训练数据本身是一个很简单的规律，但是模型太复杂了，把那些噪声和随机波动也给学会了，这该怎么办？自然就是简化一下模型的复杂度了，比如这个案例中你使用一个非常复杂的神经网络来训练效果甚至不如线性模型好，还可以增大数据量，这样模型也会变得相对简单了，但是有的时候我们确实无法手机或者说懒得收集更多的数据，怎么办？那就在原有的数据中创造更多的数据，比如在图像处理中，我们可以对图像进行旋转，翻转，裁剪，加噪声等操作</p>
<p><img src="https://photo.459122.xyz/i/968762c875ba281d9b9379f91bade928.png" alt="image-20250521210101561"></p>
<p>创造出更多的新的训练样本，这就叫做数据增强。</p>
<p>这样不仅仅能够产生更多的数据还刚好训练了一个让模型不因为输入的一点点小的变化而对结果产生很大的波动，这就是增强了模型的鲁棒性（Robostness）.</p>
<p>刚刚是从数据和模型的本身入手来防止过拟合。那有没有可能从训练过程入手组阻止过拟合的发生呢？</p>
<p>其实训练过程就是不断调整参数的过程，只要让参数不要过分的朝着过拟合的方向发展就可以了。</p>
<p><img src="https://photo.459122.xyz/i/8adc4dc6a032084207201454761b3b5f.png" alt="image-20250521210351147"></p>
<p>有一个简单到你都不相信的方法就是提前终止训练过程，差不多就行了，但是这种方法还是太粗糙了。那有没有什么方法可以直接抑制参数的野蛮增长呢？非常简单，你想想参数是怎么被训练出来的，是不是通过让参数往损失函数变小的方向不断调整，也就是梯度下降，那我们可以在损失函数中把参数本身的值加上去，这样在参数往调大了调时，如果让损失函数减小的没有那么多，导致新的损失函数反而是变大了，那么此时的调整就是不合适的，因此一定程度上一直了参数的野蛮生长</p>
<p><img src="https://photo.459122.xyz/i/dde4cf57892788f7bb15be7c7d3f3181.png" alt="image-20250521210757072"></p>
<p>除了可以用参数的绝对值之和之外，我们还可以用参数的平方和，这样参数大的时候抑制能力就更强了</p>
<p>我们把这一项叫做惩罚项，把通过这种向损失函数中添加权重惩罚项，抑制其野蛮增长的方法叫做正则化</p>
<p>上面绝对值相加的叫L1正则化，下面平方和相加的叫L2正则化</p>
<p>然后和之前梯度下降时增加学习率控制下降力度一样我们也增加一个参数来控制惩罚项的力度，我们叫它正则化系数，而这些控制参数的参数，我们以后统称为超参数</p>
<p>那为什么简简单单的公式叫什么L1正则化，L2正则化呢？因为绝对值之和叫做L1范数，而平方和的平方根叫做L2范数，这是向量空间中范数的概念</p>
<p><img src="https://photo.459122.xyz/i/4a91e7f2a427103a15a541d48e378f76.png" alt="image-20250521211051814"></p>
<p>总之这些参数都只是为了抑制参数的野蛮增长罢了</p>
<p>除了这种方式以外还有一种简单到发指但是就是效果显著的方法</p>
<p>想想看，我们的目的时为了防止让模型过于依赖某几个参数</p>
<p>举个形象的例子，加入神经网络的参数是一支军队，里面有好多普通士兵，但是其中混入了一支战斗力极强的战士</p>
<p>如果每次训练都有战士主导战局，那么你会误认为这整体战斗力很强，一旦遇到特殊情况那就会败北，这就是过度依赖少量参数的典型表现，那怎么办呢？我们可以在训练的过程中每次都随机丢弃一部分参数让战士偶尔缺席，这样模型就必须习惯去依赖大部分士兵从而避免了在某些关键参数上过度依赖的风险，虽然听起来玄学但是确实十分有效，这种方法叫dropout,这个方法是大名鼎鼎的神经学之父辛顿提出的。</p>
<p>除此之外模型还会遇到其他问题比如梯度小时，也就是网络越深，梯度反向传播时会越来越小导致参数更新困难，梯度爆炸，梯度数值越来越大，参数的调整幅度失去了控制。收敛速度过慢，可能陷入局部最优或者来回震荡，计算开销过大，数据规模量太庞大了，每次完整的前向传播和反向传播都非常耗时。每个问题，人们都想了各种办法来解决。比如用梯度裁剪来解决梯度的更新过大，用合理的网络结构比如残差网络来防止深层网络的梯度衰减，用合理的权重初始化和将输入数据归一化让梯度分布更平滑，用动量法，RMSprompt,Adam等自适应优化器来加速收敛，减少震荡，用mini-batch把巨量的训练数据分割成几个小批次来降低单次的计算开销，这里的每个概念展开都是一个全新的世界</p>
<p><img src="https://photo.459122.xyz/i/de8d52fbe515983381de48df5c4414d6.png" alt="image-20250521221946107"></p>
<p>但是它们都是为了让训练过程更好罢了</p>
<p><img src="https://photo.459122.xyz/i/7eb4772f2dbf405670651c1591aa4d56.png" alt="image-20250521223926584"></p>
<h2 id="04-神经网络中永远也搞不明白的矩阵和CNN">04 神经网络中永远也搞不明白的矩阵和CNN</h2>
<p>我们直接进入主题<br>
一个最简单的神经网络，就是Y等于WX加B套上一个激活函数<br>
那如果输入变成了两个，那么就是两个W和两个X<br>
如果输入变成了三个，那么就是三个W和三个X<br>
以此类推</p>
<p><img src="https://photo.459122.xyz/i/1efe063a04a45f05abace4262afb9110.png" alt="image-20250525231901089"></p>
<p>我就不写了<br>
那如果输出变成两个<br>
再来一行公式就可以了<br>
那这里的W的标号保证不一样<br>
能区分开就行</p>
<p><img src="https://photo.459122.xyz/i/eafecd482f4f811785444ab0c25ab9fd.png" alt="image-20250525231930237"></p>
<p>比如说这个W12<br>
就表示第一个神经元的第二个参数<br>
好你发现一个问题没有<br>
就是这样写下去的话<br>
太麻烦了<br>
要是神经元多了的话，公式密密麻麻的，没有数学的简洁之美<br>
那这怎么办呢？<br>
别急<br>
现在我们的注意力放在这个公式上<br>
注意看啊，我要变形了<br>
<img src="https://photo.459122.xyz/i/c068ef9b34518adf8c6ca9e7f34fdde1.png" alt="image-20250525232007741"></p>
<p>其实就是把加减乘除替换成了矩阵运算的写法<br>
这里先忽略一下激活函数哈<br>
重点看中间这个矩阵的乘法<br>
矩阵乘法很简单<br>
我们错个位<br>
就是这一行W的元素<br>
分别和X这一列的元素相乘<br>
并求和<br>
得到的结果呢放到这里<br>
那同样对于第二行也是如此</p>
<p>回到刚刚<br>
我们现在把这些矩阵都替换成新的字母<br>
这里我们用大写的Y表示<br>
这里用大写的W表示<br>
这里用大写的X表示<br>
这里用小写的B来表示</p>
<p><img src="https://photo.459122.xyz/i/c96b16970572eea06f6e0617e5fc4c40.png" alt="image-20250525232057332"></p>
<p>那么整个公式就化简成了这个样子</p>
<p><img src="https://photo.459122.xyz/i/12456247d70ef52fac547b1b41ce3725.png" alt="image-20250525232108728"></p>
<p>不过现在还有个问题<br>
就是神经元的层并没有体现在公式中<br>
那假如神经元再多几层怎么办呢？<br>
那我们此时抽象一下<br>
也别分什么XY和隐藏层了<br>
就通通用字母a来表示<br>
那输入层就当做第零层<br>
用A中括号零来表示<br>
以此类推</p>
<p>那么第一层的公式就是这样<br>
第二层的公式就是这样<br>
第三层的公式就是这样</p>
<p><img src="https://photo.459122.xyz/i/238a079d3c1e56019b35ce4afc03d634.png" alt="image-20250525232138692"></p>
<p>我们用L表示在第几层<br>
那么最终的通用公式就是这个样子</p>
<p><img src="https://photo.459122.xyz/i/6bcbe942a3153b2283bdb33cae9352ab.png" alt="image-20250525232157117"></p>
<p>每一层的神经元的值都是上一层的函数<br>
那我们费了这么大劲<br>
简化这个公式有啥用呢<br>
一方面是公式简单了<br>
也更抽象了,有利于我们进一步讨论更深的问题,另一方面是麻烦的加减乘除替换成了矩阵运算<br>
可以充分利用GPU的并行计算的特性,加速神经网络的训练和推理过程,这就不仅仅是秀写法上的一个操作了<br>
那回到这个公式和神经网络结构,可以看到这里的每个神经元<br>
都与前一层的所有神经元相连,当然我们一直认为这是理所应当的<br>
但它其实只是神经网络结构中的一种叫做全连接层</p>
<p><img src="https://photo.459122.xyz/i/58269cd4e857a66f36a58b96774f33c1.png" alt="image-20250525232307356"></p>
<p>也就是说还有其他不是全连接的结构吗?<br>
别急<br>
我们先来看一下全连接层的问题<br>
假如我们现在要做个图像识别的模型<br>
假如输入是个30×30的灰度图像<br>
那么平铺展开后<br>
喂给输入层的就是900个神经元<br>
假如下一层的神经元的数量是1000个<br>
那么这个全连接层的总参数量就达到了90万</p>
<p><img src="https://photo.459122.xyz/i/6fb023fc608ce1092d29a3f081b1e934.png" alt="image-20250525232333495"></p>
<p>这太大了<br>
另外呢这里仅仅是把输入的图片平铺展开<br>
无法保留像素之间的空间关系<br>
图片稍稍动一下<br>
可能所有神经元都和原来完全不同<br>
但从图片整体上看<br>
可能仅仅是平移或者变暗<br>
这就是不能很好地理解图像的局部模式<br>
那怎么办呢?<br>
我们随便在这个图像中取一个3×3的矩阵<br>
这里面的数值就是颜色的灰度值<br>
然后我们再来一个固定的矩阵<br>
比如这样把这两个矩阵进行这样的一个运算<br>
46×0加上75×-1<br>
加上82×0<br>
也就是把每个对应位置处的值相乘并求和</p>
<p><img src="https://photo.459122.xyz/i/38b4bde711496706697aa2aee85d21d3.png" alt="image-20250525232424806"></p>
<p>最终得到一个值是250<br>
然后我们再选取一个地方再次进行这样的运算<br>
最终我们把这种运算方式遍历划过原图像的每个地方<br>
得出的数值形成一个新的图像</p>
<p><img src="https://photo.459122.xyz/i/37a3574870f5220b48db80a7262eae4b.png" alt="image-20250525232443112"></p>
<p>那这种方式叫做卷积运算</p>
<p><img src="https://photo.459122.xyz/i/6873a4912b1d526ac57e2ed0021c5482.png" alt="image-20250525232513024"></p>
<p>而刚刚我们这个固定的矩阵叫做卷积核<br>
卷积核不是一个新的概念<br>
在传统的图像处理领域<br>
卷积核是已知的<br>
可以达到一定的图像处理效果<br>
比如模糊效果<br>
浮雕效果<br>
轮廓效果以及刚刚的锐化效果等等<br>
就是PS的常规操作嘛<br>
那在深度学习领域<br>
卷积核的值就是未知的<br>
和神经网络中的其他参数一样<br>
是被训练出来的一组值<br>
那回到刚刚的经典神经网络结构<br>
其实就是把其中一个全连接层替换成了卷积层<br>
这就大大的减少了权重参数的数量<br>
同时还能更有效地捕捉到<br>
图片中的一些局部特征<br>
可谓是一举两得<br>
而从公式上看</p>
<p><img src="https://photo.459122.xyz/i/b65e817439d1f23265554798ce6f20dd.png" alt="image-20250525232602503"></p>
<p>其实就是把原来的矩阵的标准乘法及差乘替换成了卷积运算<br>
那接下来我们的神经网络<br>
就不用再画成一个一个的小圈了<br>
而用更抽象更简洁的图来表示<br>
像这样在图像识别的神经网络结构中<br>
除了卷积层外<br>
通常还有池化层作用<br>
是对卷积层后的特征图像进行降维减少计算量同时呢保留主要特征<br>
这里的卷积层,池化层,全连接层都可以有多个<br>
而这种适用于图像识别领域的神经网络结构</p>
<p><img src="https://photo.459122.xyz/i/cd0d2a1a307e08cb805499aec7251c5c.png" alt="image-20250525232649478"></p>
<p>就叫做卷积神经网络CNN<br>
之前我们展示的手写数字识别的CNN可视化<br>
就是这样的网络结构,最开始是一个输入层<br>
我们写了一个数字六,然后是卷积层,池化层,再卷积层,再池化层<br>
然后第一个全连接层<br>
第二个全连接层<br>
最终输出识别出是六</p>
<p><img src="https://photo.459122.xyz/i/46419b50fcc116cf171b8b464eab24bb.png" alt="image-20250525232726717"></p>
<p>而使用卷积神经网络非常方便可视化<br>
我们可以看到训练过程中所训练出的卷积核<br>
从原始图像中提取了什么样的特征<br>
虽然这些都是中间隐藏层的事情<br>
但是却能神奇地观察出一些实际意义<br>
这也是卷积神经网络让人着迷的地方<br>
好我们来回顾一下今天讲的内容<br>
非常简单<br>
我们把之前一个一个加减乘除很麻烦的写法<br>
写成了矩阵的形式<br>
一是为了方便讨论<br>
比如刚刚介绍CNN的时候<br>
就从公式直接看出<br>
就是差乘变成了卷积运算而已<br>
二是可以更好的利用GPU的并行计算提高效率<br>
那接下来我们把之前默认的那种<br>
所有神经元都连起来的形式叫做全连接<br>
进而呢通过图像识别这个任务<br>
意识到了全世界的局限性<br>
接下来我们通过卷积运算<br>
代替了全连接层的标准矩阵乘法<br>
一方面使得训练参数大大的减少<br>
另外一方面也更有利于提取图像的局部特征<br>
这就解决了我们一开始说的问题<br>
最后我们把神经网络结构再次抽象一个层次<br>
原来我们画的各种小圈圈<br>
在更高的视角下<br>
其实就是个全连接层而已<br>
那么这些全连接层,卷积层,池化层的组合就构成了卷积神经网络CNN<br>
当然卷积神经网络CNN也只是神经网络结构中的一种<br>
而且呢它有一个致命的局限性就是它主要用于静态数据比如说图片<br>
那么如果我们要处理的是时间序列,文本,语音视频等动态数据<br>
就需要引入另外一种神经网络结构了,它可以说是现在我们大语言模型的鼻祖了<br>
好我们用了四个视频的内容<br>
终于把前面所需要铺垫的知识<br>
从头到尾给推出来了<br>
那下个视频开始<br>
我们就可以坐着我们这几个视频搭载的火箭<br>
冲刺到现代AI技术的最前沿<br>
请大家做好战斗准备</p>
<h2 id="05-语言居然可以被计算出来？从-RNN-到-Transformer">05 语言居然可以被计算出来？从 RNN 到 Transformer</h2>
<p>给你几个字<br>
让你生成下一个字<br>
给你一句话<br>
让你判断每个词的褒贬<br>
如果把这些设计成一个神经网络的函数<br>
来实现这个功能<br>
你该怎么做呢？先别急<br>
要想把这些文字作为输入参数<br>
首先得把这些文字变成计算机能够识别的数字<br>
这个过程就叫做编码<br>
那具体怎么编码呢<br>
有两种极端的方式<br>
一种是只用一个数字标识来代表每个词<br>
比如1代表我<br>
2代表你3568代表地球等等<br>
你的词表有多大<br>
数字标识的范围就要有多大<br>
这样的缺点非常明显<br>
就是维度太低了<br>
相当于一个一维的向量<br>
而且数字标识本身对语言理解没有任何意义<br>
无法灵活地衡量词和词之间的相关性<br>
那另一种极端的方式是<br>
准备一个超级超级大的向量<br>
每个词只有向量中一个位置是1<br>
剩下的都是零</p>
<p><img src="https://photo.459122.xyz/i/8345cb6afedbd2d5f8300996afb0abcb.png" alt="image-20250525233554105"></p>
<p>这种编码方式叫做one hot<br>
翻译过来叫独热编码<br>
one hot的缺点也非常明显<br>
就是维度太高了<br>
而且非常稀疏<br>
假如此表中有10万个词<br>
那么这就是一个10万维度的向量<br>
而且每个向量之间都是正交的<br>
所以词和词之间仍然无法找到相关性<br>
那如果把向量中每个位置都看作一个特征的话<br>
这里就相当于每个特征都是非常死板的<br>
是或者否维度太高不好<br>
维度太低也不好<br>
那简单了<br>
弄一个不高不低的就好喽<br>
这种方式就叫做词嵌入 word embedding<br>
通过磁嵌入的方式所得到的磁向量<br>
维度不高也不低<br>
每个位置数依然可以理解为某一个特征<br>
只不过这是训练出来的<br>
不是我们人定的</p>
<p><img src="https://photo.459122.xyz/i/1203e73a5c262a3f99d645e4bf1b1055.png" alt="image-20250525233700713"></p>
<p>所以特征是什么<br>
可能我们人类完全无法理解<br>
那为什么这种方式可以表示词和词之间<br>
语义上的相关性呢?<br>
我们可以用两个向量的点击或余弦相似度<br>
来表示向量之间的相关性<br>
进而表示两个词语之间的相关性</p>
<p><img src="https://photo.459122.xyz/i/1bf3194df3b5895cd42f29d40af0f7a6.png" alt="image-20250525233725719"></p>
<p>这就将自然语言之间的联系<br>
转化为了可以用数学公式计算出来的方式很关键<br>
同时一些数学上的计算结果<br>
也能反映出一些现实中很神奇的解释<br>
比如一个训练好的词嵌入矩阵<br>
可能会使得桌子减去椅子等于鼠标减去键盘</p>
<p><img src="https://photo.459122.xyz/i/67660d7c3638b925ae9b631cc4688efa.png" alt="image-20250525233806254"></p>
<p>你可以暂停下来<br>
体会一下这里面蕴含的有趣的深意<br>
把所有词向量组成了一个大矩阵<br>
这个大的矩阵就叫做嵌入矩阵<br>
这里的每一列就表示一个词向量<br>
像刚刚说的这个矩阵<br>
不是我们人类手动给每个词赋值而形成的<br>
是通过深度学习的方法训练出来的<br>
比如比较经典的方式就是word2back<br>
这里就不展开讲解了<br>
你就当做已经有了一个这样的嵌入矩阵<br>
每一个可能的词语<br>
都可以从这里找到对应的词向量<br>
这些词向量的维度非常高<br>
所以它所在的空间的维度也非常高<br>
这个空间就叫做潜空间<br>
我们人类对二维空间很好理解<br>
最多到三维空间也还行<br>
再往上就想象不出来了<br>
那么这些词在高维前空间中的相对位置关系<br>
虽然可以通过点击或余弦相似度算出来<br>
但最好有一种直观的方式<br>
能让我们亲眼可视化的看到<br>
哪怕不那么准确也行<br>
于是便有了一些方法将这个潜空间降维<br>
投影到二维或三维的坐标系中</p>
<p><img src="https://photo.459122.xyz/i/594e48da793c229b6a8c568994ff9920.png" alt="image-20250525233937998"></p>
<p>来直观的可视化不同词语之间的距离<br>
还是非常有趣的</p>
<p><img src="https://photo.459122.xyz/i/cf1b2e5ac33f1cfae5ee9f86b01eebdb.png" alt="image-20250525233957662"></p>
<p>好了<br>
有关词嵌入和嵌入矩阵<br>
我们就先聊到这里<br>
这时每个词都可以编码成向量<br>
然后送到神经网络输入端的神经元中了</p>
<p><img src="https://photo.459122.xyz/i/a72b71c4adbf6dbb91845526d08a45c2.png" alt="image-20250525234017983"></p>
<p>我们再来看看最初的需求：输入一句话输出每个单词的褒贬性</p>
<p><img src="https://photo.459122.xyz/i/37b2d7ff2ddd37ab18b30a02720f8d58.png" alt="image-20250525234033257"></p>
<p>这里有12345  5个词<br>
通过词嵌入<br>
把每个词变成一个300维的磁向量<br>
那么输入端就要一共有1500个神经元</p>
<p><img src="https://photo.459122.xyz/i/9e0b1c78876ee27669f7f94c85f95f02.png" alt="image-20250525234106849"></p>
<p>这样行不行呢<br>
当然可以<br>
但是有两个问题<br>
一个是输入层太大了<br>
而且会随着一句话中词语数量多少而变化<br>
是变长的，不确定的<br>
另一个是无法体现词语的先后顺序<br>
仅仅是把它们非常生硬的平铺展开成了一个非常大的向量，一股脑地送入了输入层<br>
这就好比我们之前说的图像识别领域<br>
把一张图片的所有像素点展开成一个大向量<br>
一股脑地送入输入层<br>
一个道理<br>
这样既增加了神经元的个数<br>
又不能很好地抽象出特征和关联<br>
有点费力不讨好<br>
那在CNN中<br>
我们是通过卷积操作提取了图像的特征<br>
那么在自然语言处理领域<br>
我们可以通过什么办法？<br>
既能解决词语之间的先后顺序问题<br>
又能降低输入层的参数量呢？<br>
首先我们还是用经典的神经网络<br>
但不要输入一句话<br>
而是输入一个词<br>
输出就是这个词是褒义还是贬义<br>
当然这里的字母都表示矩阵就不再赘述了<br>
这时假设第二个词来了<br>
也是经过一样的神经网络，很简单<br>
那此时我们用尖括号表示是第几个词<br>
这样就有了顺序关系<br>
那现在的问题是<br>
第二个词的计算过程<br>
完全没有让第一个词的任何信息参与进来<br>
那这该怎么办呢</p>
<p><img src="https://photo.459122.xyz/i/6ac79c6a1dc41584acf94178d83637fc.png" alt="image-20250525234241853"></p>
<p>答案已经写在脸上了<br>
那就让他参与进来就好喽<br>
那可以这样<br>
我们让第一个词经过非线性变换后<br>
别急着直接输出<br>
结果先输出到一个隐藏状态H1<br>
然后再经过一次非线性变换得到输出Y1<br>
接下来这个隐藏状态H1的值和第二个词X2<br>
一起参与运算<br>
那同理对第二个词的流程也是一样<br>
先输出一个隐藏状态H2<br>
然后继续往下传递<br>
那这样的话呢<br>
前面的词的信息就这样不断的往下传递</p>
<p><img src="https://photo.459122.xyz/i/0c57e9eda103fd024b4f3dadcf245a62.png" alt="image-20250525234322191"></p>
<p>直到传到最后一句话的最后一个词那里<br>
这样就把一句话中<br>
所有的词的信息都囊括进来了<br>
当然这里的W就要有所区分了</p>
<p><img src="https://photo.459122.xyz/i/18ce7b083ecc01d4492d9f90b0c64204.png" alt="image-20250525234355487"></p>
<p>有专门针对磁向量的WXH矩阵<br>
有专门针对隐藏状态的WHH矩阵<br>
以及最终计算输出结果的WHY矩阵<br>
那同样对于偏执向B也是如此<br>
把这个图简化一下<br>
那这就是循环神经网络RNN</p>
<p><img src="https://photo.459122.xyz/i/6dc595a7e4d8a35ad8b5db41a3b3b8f2.png" alt="image-20250525234409069"></p>
<p>当然了<br>
还会有个图这样画<br>
那这个RN模型就具备了理解词和词之间<br>
先后顺序的能力<br>
那这样就可以解决<br>
判断一句话中各个单词的褒贬词性<br>
给出一句话<br>
不断生成下一个字<br>
以及翻译等多种自然语言处理的工作了<br>
那如果你还有些懵的话<br>
我们再把矩阵展开来看看<br>
首先第一个词X1和权重矩阵WXH相乘<br>
得到第一个词的隐藏状态<br>
H1准备往后传<br>
H1和权重矩阵WHY相乘<br>
得到第一个词的输出结果Y1这时候计算第二个词<br>
同样要和权重矩阵WXH相乘<br>
但注意这个时候要把第一个词的隐藏状态加到输入向量里拼接起来<br>
同时权重矩阵也增加一个WHH最终计算出第二个词的隐藏状态H2准备继续往后传<br>
那后面的流程就一样了</p>
<p><img src="https://photo.459122.xyz/i/06b386000e882e46a76c09a5dc0a46c9.png" alt="image-20250525234544512"></p>
<p>最后看一下公式<br>
其实非常简单<br>
和经典的神经网络相比</p>
<p><img src="https://photo.459122.xyz/i/16856c40a5c50dfc02ca8ebfd542a493.png" alt="image-20250525234558252"></p>
<p>就是多了一个前一时刻的隐藏状态而已<br>
回顾一下<br>
其实本期的内容非常简单<br>
我们想处理自然语言的一系列问题<br>
首先就要把词转换成计算机能够识别的数字<br>
这个过程叫编码<br>
通过编码词而形成的向量叫做词向量<br>
编码词向量有多种方式<br>
其中一种是准备一个词表大小的向量<br>
只有一个位置是一<br>
这种方式叫做one hot及独热编码<br>
这种编码方式维度太高<br>
词之间缺乏相关性<br>
所以另一种更有效的方式叫做词嵌入<br>
词嵌入所需要经过训练而得到的矩阵<br>
叫做嵌入矩阵<br>
磁向量之间的相关性<br>
可以用点击或余弦相似度来计算<br>
有了磁向量之后<br>
就可以输入到神经网络进行各种训练了<br>
经典的神经网络无法表达词的先后顺序<br>
因此我们增加了一个隐藏状态<br>
在词和词之间传递，不同的词<br>
使用不同的时间步T来表示<br>
那这个不同于经典神经网络的结构<br>
就叫做循环神经网络RNN<br>
当然RNN还有两个非常严重的问题<br>
1.信息会随着时间步的增多而逐渐丢失<br>
无法捕捉长期依赖<br>
而有的语句恰恰是距离很远的地方<br>
起到了关键性的作用<br>
2.RNN必须按顺序处理<br>
每个时间步依赖上一个时间步的隐藏状态<br>
的计算结果<br>
那为了解决这些问题<br>
人们使用GRU和LSTM改进了传统的RNN</p>
<p><img src="https://photo.459122.xyz/i/1f020846a78f0f08e5c016e2512c1d12.png" alt="image-20250525234737971"></p>
<p>但是这些仍然是建立在让信息一点一点<br>
按照时间簿传递的思路来解决<br>
只能缓解而无法根治<br>
那我们是否有一种可以彻底抛弃这种顺序计算<br>
直接一眼把全部信息尽收眼底的新方案呢<br>
有的那就是transformer</p>
<h2 id="06-Transformer-其实是个简单到令人困惑的模型">06 Transformer 其实是个简单到令人困惑的模型</h2>
<p>用神经网络做个翻译任务<br>
I love you Baby<br>
我爱你宝贝儿<br>
先用词嵌入的方式把每个词转换成一个词向量<br>
简单点<br>
假设维度就是六<br>
如果把每个词直接丢到一个全连接神经网络中<br>
那每个词都没有上下文的信息且长度只能一一对应<br>
不太行<br>
如果用循环神经网络RNN又面临串行计算<br>
而且如果句子太长<br>
也会导致长期依赖困难的问题也不太行<br>
那这也不行<br>
那也不行<br>
可咋整呢?<br>
小孩子才做选择<br>
成年人全都不要直接发明一个全新的方案<br>
跟我走<br>
首先我们给每个词一个位置编码<br>
表示这个词出现在整个句子中的位置<br>
具体怎么计算<br>
再说</p>
<p>把位置编码加到原来的词向量里<br>
现在这个词就有了位置信息<br>
但此时每个词还没有其他词的上下文信息<br>
也就是注意不到其他词的存在<br>
那怎么办呢<br>
接着看<br>
别眨眼</p>
<blockquote>
<p>QKV分别是查询（query）、键（key）、值（value），刚刚问大模型是这么回答的，不清楚有没有错误，明白原理的大佬帮忙指正: 1. Query：当前需要处理的词（比如翻译到中文的「苹果」）。 2. Key：句子中每个词的「身份标识」（比如英文单词 “apple”、“red”、“eat” 的含义特征）。 3. Value：这些词实际携带的语义信息（比如 “apple” 对应水果、公司等含义）。</p>
</blockquote>
<p>首先我们用一个WQ矩阵和第一个词向量相乘<br>
得到维度不变的Q1向量<br>
这里的WQ矩阵是可以通过训练过程学习的一组权重值<br>
同理我们用wk矩阵和第一个词向量相乘得到K1<br>
再用WV矩阵得到V1<br>
接着对其他词向量也和相同的WQKV矩阵相乘<br>
分别得到自己对应的QKV1向量</p>
<p><img src="https://photo.459122.xyz/i/de39a66bd96c37fb8392ba0fb7677abf.png" alt="image-20250525235101479"></p>
<p><img src="https://photo.459122.xyz/i/5bb0cf0eedb836853ed7b6fac6cb02ef.png" alt="image-20250525235117721"></p>
<p>当然实际在计算机GPU中运算的时候<br>
是通过拼接而成的大矩阵做乘法<br>
并不是像我们刚刚那样一步一步计算的<br>
得到的直接就是包含所有词向量的QKV矩阵</p>
<p>不过为了理解<br>
我们解释的时候还是拆成一个个的词向量<br>
现在原来的词向量已经分别通过线性变换<br>
映射成了QKV</p>
<p>维度和原来是一样的<br>
接下来我们让Q1和K2做点击<br>
这表示在第一个词的视角里<br>
第一个词和第二个词的相似度是多少<br>
同理依次和K3做点击表示和第三个词的相似度<br>
和K4做点击表示和第四个词的相似度<br>
最后呢也补上一个和自己做点击表示和自己的相似度<br>
那拿到这些相似度的系数后</p>
<p><img src="https://photo.459122.xyz/i/7b9a9b9fa0978dda0b027331a61e707c.png" alt="image-20250525235224109"></p>
<p>分别和V向量相乘</p>
<p><img src="https://photo.459122.xyz/i/c6895b291c7310388cb259c0291e1e04.png" alt="image-20250525235241089"></p>
<p>再相加得到A1<br>
那此时这个A1就表示在第一个词的视角下<br>
按照和它相似度大小按权重把每个词的词向量都加到了一块儿<br>
那这就把全部上下文的信息都包含在第一个词当中了而且是用第一个词的视角来看的<br>
同理其他几个词也按照这种方式，那么此时每个词都把其他词的词向量按照和自己的相似度权重加到了自己的词向量中</p>
<p>好<br>
那这里的什么QKV都是中间的计算过程了<br>
我们从全局视角看<br>
现在就是把最初的输入的词向量经过一番处理<br>
变成了一组新的词向量<br>
不一样的是呢<br>
这组新的词向量中<br>
每一个都是包含了位置信息和其他词<br>
上下文信息的一组新的词向量</p>
<p><img src="https://photo.459122.xyz/i/ab87d38e05b67fc6c53f95e1b070b83a.png" alt="image-20250525235334042"></p>
<p>这就是注意力机制attention做的事情<br>
我们再进一步优化下<br>
有的时候一个词和另一个词的关系<br>
可能从不同的视角看是不一样的<br>
对于注意力机制来说，如果只通过一种方式计算一次相关性<br>
灵活性就会大大降低<br>
所以我们做些改进<br>
之前我们是每个词计算一组QKV<br>
现在我们在这个QKV基础上<br>
再经过两个权重矩阵变成两组QKV<br>
给每个词两个学习机会</p>
<p><img src="https://photo.459122.xyz/i/7e8682627c6b4adeb6c039d7359ebdb7.png" alt="image-20250525235414626"></p>
<p>学习到不同的要计算相似度QQV<br>
来增加语言的灵活性<br>
这里的每组QKV成为一个头 head<br>
接下来在每个头里面的QQV<br>
仍然经过刚刚的注意力层的运算<br>
得到A向量<br>
然后把两个A向量拼接起来<br>
得到了和刚刚一样的结构<br>
而对于刚刚的注意力机制attention<br>
这种方式就叫做多头注意力Multi head attention<br>
而我们刚刚举的例子就是两个头的情况</p>
<p>好<br>
那我现在要恭喜你<br>
已经把transformer架构<br>
最核心的逻辑都搞清楚了</p>
<p><img src="https://photo.459122.xyz/i/ead1b8bb5cd9902d9881b6329ca00902.png" alt="image-20250525235456217"></p>
<p>你信不信不信的话<br>
我们对照一下transformer的经典论文<br>
中的架构图来看看<br>
首先第一步就是把输入的内容<br>
通过词嵌入的方式转换成词向量矩阵<br>
对应的就是这里<br>
第二步加入位置信息<br>
其实就是再加个形状一样的矩阵<br>
对应的就是这里<br>
第三步经过多头注意力的处理<br>
输出的矩阵维度和输入没有变化<br>
给每个词向量增加了上下文信息<br>
对应的就是这里<br>
后面还有一步添加了残差网络和归一化处理是为了解决梯度消失并且让分布更加稳定而做的优化<br>
我们刚刚没有展开这块儿<br>
那对应的就是这里<br>
同时我们也可以看到<br>
整个transformer的标准架构中<br>
最主要的就是多头注意力的处理<br>
相当于我们把这些部分的逻辑都搞明白了<br>
快给自己鼓鼓掌吧<br>
下面深入到多头注意力机制的细节部分<br>
我们再看看</p>
<p><img src="https://photo.459122.xyz/i/7719c96f2755296647df2dcb100c40db.png" alt="image-20250525235609581"></p>
<p>如果是不分多头的单头注意力<br>
那么就是先让Q和K相乘<br>
得到一个相似度系数的一个矩阵<br>
然后再和V相乘<br>
最终得到了包含上下文信息的磁向量矩阵<br>
省略了中间的缩放掩码和一层soft max处理<br>
再看右边的多头注意力情况</p>
<p>首先QKV分别经过线性变换<br>
拆分成多组<br>
相当于给了多次机会学习到不同的相似度关系<br>
依次经过注意力机制运算后<br>
把运算结果拼接起来<br>
是不是完全一样呢<br>
不过我们讲解的时候还省略了一次线性变换<br>
即多头结果并不是简单的拼接起来<br>
还需要再次经过一层权重矩阵的乘法<br>
这时候再看两个核心公式就很好理解了<br>
所谓注意力运算就是QK矩阵相乘经过缩放<br>
在经过soft max层处理</p>
<p><img src="https://photo.459122.xyz/i/e872a51a8bb7221e24422e0e9a047849.png" alt="image-20250525235647356"></p>
<p>最后和V相乘<br>
对于多头情况<br>
就是先将QKV矩阵<br>
经过多个权重矩阵拆分到多个头中<br>
分别经过注意力机制的运算<br>
最后合并起来<br>
再经过一次矩阵运算<br>
得到了输出</p>
<p>再回过头来看下这个全局的图<br>
左边的部分叫做编码器<br>
右边的部分叫做解码器</p>
<p>你实在不愿意叫也没关系<br>
假设这个是用于翻译的任务<br>
我们训练这个神经网络的过程是<br>
首先输入要翻译的文本<br>
I love you baby<br>
然后经过词嵌入引入位置编码<br>
经过多头注意力残差和归一化处理<br>
接着送入一个全连接神经网络<br>
再残差和归一化处理<br>
结果送入解码器的一个多头注意力机制的两个输入中作为KV矩阵<br>
再看右边解码器的部分输出是翻译后的文本<br>
我爱你宝贝儿<br>
同样经过此嵌入<br>
引入位置编码<br>
经过多头注意力<br>
然后残差和归一化处理下<br>
然后送入上面说的多头注意力的一个输入中<br>
作为Q矩阵和刚刚从编码器中送入的KV矩阵<br>
再经过多头注意力残差归一化<br>
再全连接神经网络<br>
再残差归一化<br>
最后再经过一层线性变换的神经网络<br>
投射到此表向量中<br>
最后用soft max层转化为概率<br>
这就代表预测的下一个词在词表中的概率分布<br>
那我们取概率最高的就是下一个词应该是什么<br>
这里有一个小的不同<br>
就是有个掩码</p>
<p><img src="https://photo.459122.xyz/i/7c99c4160e2be201f2acc967d7b91744.png" alt="image-20250525235842641"></p>
<p>这个掩码的作用是真正推理<br>
翻译时是一个词一个词翻译的<br>
比如说这个时候翻译到我下一个词应该是爱<br>
所以输出我的时候是看不到后面的词的<br>
这就需要掩码来把后面的词遮挡住<br>
以便训练的时候模拟真实推理场景时的过程<br>
比如当此时输入是i love you baby<br>
输出只有一个词<br>
我的时候经过这个神经网络<br>
最后上方输出词表的概率分布<br>
我们想要的结果就是I字的概率值最大<br>
如果训练时有偏差<br>
那么就计算损失函数<br>
再反向传播<br>
调整transformer结构中的各种权重矩阵<br>
直到学习好为止<br>
总的来说<br>
transformer确实是个特别简单的架构<br>
原文中也是这样说的<br>
尤其是当你有了基础的神经网络知识之后<br>
如果你看了这个系列之前的视频<br>
那就只有多头注意力<br>
这一层是陌生的<br>
但是它其实拆解之后也是各种矩阵相乘呀<br>
相加呀<br>
这种操作罢了<br>
那其余的词嵌入位置<br>
编码残差归一化<br>
经典神经网络<br>
soft max层等等<br>
都是我们之前的视频中已经了解过的概念<br>
把这些老东西拼凑拼凑就诞生了<br>
我们现在大模型技术的鼻祖transformer<br>
那GBT的底层其实就是transform的一半<br>
即只有解码器的部分<br>
也不翻译<br>
谁只管看前面的词<br>
猜下一个词<br>
别看他来回的猜词猜词<br>
猜着猜着就变成了聊天写代码<br>
解数学题的全能选手了<br>
那transformer的架构来源于经典的论文<br>
Attention is all you need<br>
本期视频的内容理解之后再去看这篇论文<br>
你就会发现非常非常非常容易理解<br>
因为它本身就是一个很简单的架构<br>
也正因为简单粗暴<br>
但是效果却出奇的好<br>
所以才会广为流传<br>
并成为现代大模型的基础</p>
<p>本文内容源自B站系列视频 <a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1wXQhY8EJ6?vd_source=b0f2742e7b4a2838ad4d1870af693bc1">https://www.bilibili.com/video/BV1wXQhY8EJ6?vd_source=b0f2742e7b4a2838ad4d1870af693bc1</a></p>
</article><div class="post-copyright"><div class="post-copyright__title"><span class="post-copyright-info"><h>神经网络</h></span></div><div class="post-copyright__type"><span class="post-copyright-info"><a href="https://yjyrichard.github.io/posts/7ca31f7.html">https://yjyrichard.github.io/posts/7ca31f7.html</a></span></div><div class="post-copyright-m"><div class="post-copyright-m-info"><div class="post-copyright-a"><h>作者</h><div class="post-copyright-cc-info"><h>Yangjiayu</h></div></div><div class="post-copyright-c"><h>发布于</h><div class="post-copyright-cc-info"><h>2025-05-26</h></div></div><div class="post-copyright-u"><h>更新于</h><div class="post-copyright-cc-info"><h>2025-05-29</h></div></div><div class="post-copyright-c"><h>许可协议</h><div class="post-copyright-cc-info"><a class="icon" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a><a rel="noopener" target="_blank" title="CC BY-NC-SA 4.0" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a></div></div></div></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%A7%91%E6%99%AE/"><div class="tags-punctuation"><svg class="faa-tada icon" style="height:1.1em;width:1.1em;fill:currentColor;position:relative;top:2px;margin-right:3px" aria-hidden="true"><use xlink:href="#icon-sekuaibiaoqian"></use></svg></div>科普</a></div></div><link rel="stylesheet" href="/css/coin.css" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">投喂作者</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://bilibili123.oss-cn-beijing.aliyuncs.com/about/%E5%BE%AE%E4%BF%A1%E6%94%AF%E4%BB%98.jpg" target="_blank"><img class="post-qr-code-img" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/about/%E5%BE%AE%E4%BF%A1%E6%94%AF%E4%BB%98.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="https://bilibili123.oss-cn-beijing.aliyuncs.com/about/%E6%94%AF%E4%BB%98%E5%AE%9D.jpg" target="_blank"><img class="post-qr-code-img" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/about/%E6%94%AF%E4%BB%98%E5%AE%9D.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></button></div><audio id="coinAudio" src="https://npm.elemecdn.com/akilar-candyassets@1.0.36/audio/aowu.m4a"></audio><script defer="defer" src="/js/coin.js"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/81c54482.html"><img class="prev-cover" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/13.png" onerror="onerror=null;src='/assets/r2.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习期末设计</div></div></a></div><div class="next-post pull-right"><a href="/posts/8d03c736.html"><img class="next-cover" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/15.png" onerror="onerror=null;src='/assets/r2.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">计算机组成原理之概述篇</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/64d0aedf.html" title="Github"><img class="cover" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/15.png" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2025-04-15</div><div class="title">Github</div></div></a></div><div><a href="/posts/90.html" title="搞懂URI和URL"><img class="cover" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/5.png" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2025-02-22</div><div class="title">搞懂URI和URL</div></div></a></div><div><a href="/posts/8610680.html" title="大模型的token究竟是啥?"><img class="cover" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/6.png" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2025-03-12</div><div class="title">大模型的token究竟是啥?</div></div></a></div><div><a href="/posts/2aa61245.html" title="数字签名和CA数字证书的核心原理和作用"><img class="cover" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/4.png" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2025-04-15</div><div class="title">数字签名和CA数字证书的核心原理和作用</div></div></a></div><div><a href="/posts/d1d9e1a.html" title="你的皮肤是如何精心设计的？它为何是人体最大的器官？【皮肤真相】"><img class="cover" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/4.png" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2025-05-16</div><div class="title">你的皮肤是如何精心设计的？它为何是人体最大的器官？【皮肤真相】</div></div></a></div><div><a href="/posts/e45bd8c5.html" title="编程是什么"><img class="cover" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/9.png" alt="cover"><div class="content is-center"><div class="date"><i class="fas fa-history fa-fw"></i> 2025-03-12</div><div class="title">编程是什么</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><svg class="meta_icon" style="width:22px;height:22px;position:relative;top:5px"><use xlink:href="#icon-mulu1"></use></svg><span style="font-weight:bold">目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#01-%E4%BB%8E%E5%87%BD%E6%95%B0%E5%88%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-text">01 从函数到神经网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#02-%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%8F%82%E6%95%B0"><span class="toc-text">02 如何计算神经网络的参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#03-%E8%B0%83%E6%95%99%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%80%8E%E4%B9%88%E5%B0%B1%E8%BF%99%E4%B9%88%E9%9A%BE%E5%91%A2%EF%BC%9F"><span class="toc-text">03 调教神经网络怎么就这么难呢？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#04-%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%AD%E6%B0%B8%E8%BF%9C%E4%B9%9F%E6%90%9E%E4%B8%8D%E6%98%8E%E7%99%BD%E7%9A%84%E7%9F%A9%E9%98%B5%E5%92%8CCNN"><span class="toc-text">04 神经网络中永远也搞不明白的矩阵和CNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#05-%E8%AF%AD%E8%A8%80%E5%B1%85%E7%84%B6%E5%8F%AF%E4%BB%A5%E8%A2%AB%E8%AE%A1%E7%AE%97%E5%87%BA%E6%9D%A5%EF%BC%9F%E4%BB%8E-RNN-%E5%88%B0-Transformer"><span class="toc-text">05 语言居然可以被计算出来？从 RNN 到 Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#06-Transformer-%E5%85%B6%E5%AE%9E%E6%98%AF%E4%B8%AA%E7%AE%80%E5%8D%95%E5%88%B0%E4%BB%A4%E4%BA%BA%E5%9B%B0%E6%83%91%E7%9A%84%E6%A8%A1%E5%9E%8B"><span class="toc-text">06 Transformer 其实是个简单到令人困惑的模型</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-color: transparent;"><div id="footer-wrap"><div id="ft"><div class="ft-item-1"><div class="t-top"><div class="t-t-l"><p class="ft-t t-l-t">格言🧬</p><div class="bg-ad"><div>再看看那个光点，它就在这里，这是家园，这是我们 —— 你所爱的每一个人，你认识的一个人，你听说过的每一个人，曾经有过的每一个人，都在它上面度过他们的一生✨</div><div class="btn-xz-box"><a class="btn-xz" href="#">点击开启星辰之旅</a></div></div></div><div class="t-t-r"><p class="ft-t t-l-t">猜你想看💡</p><ul class="ft-links"><li><a href="/social/link/">我的朋友</a><a href="/comments/">留点什么</a></li><li><a href="/personal/about/">关于作者</a><a href="/archives/">文章归档</a></li><li><a href="/categories/">文章分类</a><a href="/tags/">文章标签</a></li><li><a href="/box/Gallery/">我的画廊</a><a href="/personal/bb/">我的唠叨</a></li><li><a href="/site/time/">建设进程</a><a href="/site/census/">网站统计</a></li></ul></div></div></div><div class="ft-item-2"><p class="ft-t">推荐友链⌛</p><div class="ft-img-group"></div></div></div><div class="copyright"><span><b>&copy;2025</b></span><span><b>&nbsp;&nbsp;By Yangjiayu</b></span></div><div id="workboard"></div><p id="ghbdages"><a class="github-badge" target="_blank" href="https://hexo.io/" style="margin-inline:5px" title="博客框架为Hexo_v6.3.0"><img src="https://sourcebucket.s3.ladydaily.com/badge/Frame-Hexo-blue.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://butterfly.js.org/" style="margin-inline:5px" title="主题版本Butterfly_v4.3.1"><img src="https://sourcebucket.s3.ladydaily.com/badge/Theme-Butterfly-6513df.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://vercel.com/" style="margin-inline:5px" title="本站采用多线部署，主线路托管于Vercel"><img src="https://sourcebucket.s3.ladydaily.com/badge/Hosted-Vercel-brightgreen.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://user.51.la/" style="margin-inline:5px" title="本站数据分析得益于51la技术支持"><img src="https://sourcebucket.s3.ladydaily.com/badge/Analytics-51la-3db1eb.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://icp.gov.moe/?keyword=20226665" style="margin-inline:5px" title="本站已加入萌ICP豪华套餐，萌ICP备20226665号"><img src="https://sourcebucket.s3.ladydaily.com/badge/萌ICP备-20226665-fe1384.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://bitiful.dogecast.com/buckets" style="margin-inline:5px" title="本网站经Service Worker分流至缤纷云对象存储"><img src=" https://sourcebucket.s3.ladydaily.com/badge/Bucket-缤纷云-9c62da.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://www.netdun.net/" style="margin-inline:5px" title="本站使用网盾星球提供CDN加速与防护"><img src="https://sourcebucket.s3.ladydaily.com/badge/CDN-网盾星球-fff2cc.svg" alt=""/></a><a class="github-badge" target="_blank" href="https://github.com/" style="margin-inline:5px" title="本网站源码由Github提供存储仓库"><img src=" https://sourcebucket.s3.ladydaily.com/badge/Source-Github-d021d6.svg" alt=""/></a></p></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button><button class="share" type="button" title="右键模式" onclick="changeMouseMode()"><i class="fas fa-mouse"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog right_side"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button class="share" type="button" title="分享链接" onclick="share()"><i class="fas fa-share-nodes"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i><span id="percent">0<span>%</span></span></button><button id="go-down" type="button" title="直达底部" onclick="btf.scrollToDest(document.body.scrollHeight, 500)"><i class="fas fa-arrow-down"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div class="js-pjax" id="rightMenu"><div class="rightMenu-group rightMenu-small"><a class="rightMenu-item" href="javascript:window.history.back();"><i class="fa fa-arrow-left"></i></a><a class="rightMenu-item" href="javascript:window.history.forward();"><i class="fa fa-arrow-right"></i></a><a class="rightMenu-item" href="javascript:window.location.reload();"><i class="fa fa-refresh"></i></a><a class="rightMenu-item" href="javascript:rmf.scrollToTop();"><i class="fa fa-arrow-up"></i></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:rmf.copySelect();"><i class="fa fa-copy"></i><span>复制</span></a><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();"><i class="fa fa-search"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-too"><a class="rightMenu-item" href="javascript:window.open(window.getSelection().toString());window.location.reload();"><i class="fa fa-link"></i><span>转到链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-paste"><a class="rightMenu-item" href="javascript:rmf.paste()"><i class="fa fa-copy"></i><span>粘贴</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-post"><a class="rightMenu-item" href="#post-comment"><i class="fas fa-comment"></i><span>空降评论</span></a><a class="rightMenu-item" href="javascript:rmf.copyWordsLink()"><i class="fa fa-link"></i><span>复制本文地址</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-to"><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>新窗口打开</span></a><a class="rightMenu-item" id="menu-too" href="javascript:rmf.open()"><i class="fa fa-link"></i><span>转到链接</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制链接</span></a></div><div class="rightMenu-group rightMenu-line hide" id="menu-img"><a class="rightMenu-item" href="javascript:rmf.saveAs()"><i class="fa fa-download"></i><span>保存图片</span></a><a class="rightMenu-item" href="javascript:rmf.openWithNewTab()"><i class="fa fa-window-restore"></i><span>在新窗口打开</span></a><a class="rightMenu-item" href="javascript:rmf.copyLink()"><i class="fa fa-copy"></i><span>复制图片链接</span></a></div><div class="rightMenu-group rightMenu-line"><a class="rightMenu-item" href="javascript:randomPost()"><i class="fa fa-paper-plane"></i><span>随便逛逛</span></a><a class="rightMenu-item" href="javascript:switchNightMode();"><i class="fa fa-moon"></i><span>昼夜切换</span></a><a class="rightMenu-item" href="/personal/about/"><i class="fa fa-info-circle"></i><span>关于博客</span></a><a class="rightMenu-item" href="javascript:toggleWinbox();"><i class="fas fa-cog"></i><span>美化设置</span></a><a class="rightMenu-item" href="javascript:rmf.fullScreen();"><i class="fas fa-expand"></i><span>切换全屏</span></a><a class="rightMenu-item" href="javascript:window.print();"><i class="fa-solid fa-print"></i><span>打印页面</span></a></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.staticfile.org/fancyapps-ui/4.0.31/fancybox.umd.min.js"></script><script src="https://lf3-cdn-tos.bytecdntp.com/cdn/expire-1-M/instant.page/5.1.0/instantpage.min.js" type="module"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/node-snackbar/0.1.16/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script async="async">var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())
setTimeout(function(){preloader.endLoading();}, 5000);
document.getElementById('loading-box').addEventListener('click',()=> {preloader.endLoading()})</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://www.312782.xyz/',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://www.312782.xyz/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.staticfile.org/twikoo/1.6.8/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async src="https://cdn1.tianli0.top/npm/vue@2.6.14/dist/vue.min.js"></script><script async src="https://cdn1.tianli0.top/npm/element-ui@2.15.6/lib/index.js"></script><script async src="https://cdn.bootcdn.net/ajax/libs/clipboard.js/2.0.11/clipboard.min.js"></script><script defer type="text/javascript" src="https://cdn1.tianli0.top/npm/sweetalert2@8.19.0/dist/sweetalert2.all.js"></script><script async src="//npm.elemecdn.com/pace-js@1.2.4/pace.min.js"></script><script defer src="https://cdn1.tianli0.top/gh/nextapps-de/winbox/dist/winbox.bundle.min.js"></script><script async src="//at.alicdn.com/t/c/font_3586335_hsivh70x0fm.js"></script><script async src="//at.alicdn.com/t/c/font_3636804_gr02jmjr3y9.js"></script><script async src="//at.alicdn.com/t/c/font_3612150_kfv55xn3u2g.js"></script><script async src="https://cdn.wpon.cn/2022-sucai/Gold-ingot.js"></script><canvas id="universe"></canvas><canvas id="snow"></canvas><script defer src="/js/fomal.js"></script><link rel="stylesheet" href="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://lf6-cdn-tos.bytecdntp.com/cdn/expire-1-M/aplayer/1.10.1/APlayer.min.js"></script><script src="https://cdn1.tianli0.top/npm/js-heo@1.0.12/metingjs/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><a class="magnet_link_more"  href="https://yjyrichard.github.io/categories/" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(33.333333333333336% - 5px);background: #e9e9e9;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: var(--text-bg-hover)}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/d522d575.html" alt=""><img width="48" height="48" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/11.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-15</span><a class="blog-slider__title" href="posts/d522d575.html" alt="">小程序起步</a><div class="blog-slider__text">初识小程序</div><a class="blog-slider__button" href="posts/d522d575.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/8610680.html" alt=""><img width="48" height="48" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/6.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-15</span><a class="blog-slider__title" href="posts/8610680.html" alt="">大模型的token究竟是啥?</a><div class="blog-slider__text">大模型的token究竟是啥?</div><a class="blog-slider__button" href="posts/8610680.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/5c2f4951.html" alt=""><img width="48" height="48" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/8.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-15</span><a class="blog-slider__title" href="posts/5c2f4951.html" alt="">基础数据结构(3)</a><div class="blog-slider__text">栈，队列，堆，二叉树</div><a class="blog-slider__button" href="posts/5c2f4951.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/45347810.html" alt=""><img width="48" height="48" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/15.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-15</span><a class="blog-slider__title" href="posts/45347810.html" alt="">基础数据结构(2)</a><div class="blog-slider__text">递归</div><a class="blog-slider__button" href="posts/45347810.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/3ec27523.html" alt=""><img width="48" height="48" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/10.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-21</span><a class="blog-slider__title" href="posts/3ec27523.html" alt="">微信支付</a><div class="blog-slider__text">java整合微信的网页支付包含原理</div><a class="blog-slider__button" href="posts/3ec27523.html" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="posts/6e192bd3.html" alt=""><img width="48" height="48" src="https://bilibili123.oss-cn-beijing.aliyuncs.com/websitepic/7.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-04-15</span><a class="blog-slider__title" href="posts/6e192bd3.html" alt="">基础数据结构(1)</a><div class="blog-slider__text">数组与链表</div><a class="blog-slider__button" href="posts/6e192bd3.html" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><div class="js-pjax"><script async="async">var arr = document.getElementsByClassName('recent-post-item');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('blog-slider swiper-container-fade swiper-container-horizontal');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('catalog_magnet');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '1.5s·');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('card-widget');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__zoomIn');
    arr[i].setAttribute('data-wow-duration', '2s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script><script async="async">var arr = document.getElementsByClassName('pagination');
for(var i = 0;i<arr.length;i++){
    arr[i].classList.add('wow');
    arr[i].classList.add('animate__jackInTheBox');
    arr[i].setAttribute('data-wow-duration', '1.5s');
    arr[i].setAttribute('data-wow-delay', '200ms');
    arr[i].setAttribute('data-wow-offset', '30');
    arr[i].setAttribute('data-wow-iteration', '1');
  }</script></div><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow.min.js"></script><script defer src="https://npm.elemecdn.com/hexo-butterfly-wowjs/lib/wow_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('gitZone');
      var item_html = '<div class="recent-post-item" id="gitcalendarBar" style="width:100%;height:auto;padding:10px;"><style>#git_container{min-height: 320px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></div>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('gitZone') && (location.pathname ==='/site/census/'|| '/site/census/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("/api?null",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'null')
    }
  </script><!-- hexo injector body_end end --></body></html>